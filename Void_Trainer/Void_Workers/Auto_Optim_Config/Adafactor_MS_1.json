{
    "optimizer_name": "Adafactor",
    "learning_rate_scheduler": "Multistep",
    "milestone": [4, 8, 12],
    "gamma": 0.5,
    "learning_rate": 2e-4,
    "weight_decay": 0.05,
    "decay_rate": -1.0,
    "gradient_clip": 1.0,
    "scale_parameter": "false",
    "relative_step": "false",
    "warmup_init": "false"
}